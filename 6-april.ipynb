{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7851d473-c824-4011-94ad-89d2a9aa242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "Ans-\n",
    "    A linear Support Vector Machine (SVM) is a type of machine learning algorithm that is commonly used for classification tasks. The mathematical formula for a linear SVM can be written as:\n",
    "\n",
    "         y = w^T x + b\n",
    "\n",
    "where:\n",
    "\n",
    "y is the predicted class label for the input sample x\n",
    "w is the weight vector\n",
    "b is the bias term\n",
    "The goal of the linear SVM is to find the weight vector w and the bias term b that will maximize the margin between the decision boundary and the closest data points from both classes. The decision boundary is the hyperplane that separates the data points of different classes.\n",
    "\n",
    "To train a linear SVM, we use an optimization algorithm to minimize the objective function:\n",
    "\n",
    "minimize (1/2) ||w||^2\n",
    "subject to y_i(w^T x_i + b) >= 1 for all i\n",
    "where:\n",
    "\n",
    "||w|| is the Euclidean norm of the weight vector\n",
    "y_i is the class label for the i-th training sample\n",
    "x_i is the i-th training sample\n",
    "The optimization problem is subject to the constraint that all training samples are classified correctly with a margin of at least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b95b73-e587-42b7-b42c-08cf25d829a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the objective function of a linear SVM?\n",
    "Ans-\n",
    "   The objective function of a linear Support Vector Machine (SVM) is to find the optimal hyperplane that separates the input data into two classes with the maximum margin between them. The margin is the distance between the hyperplane and the closest points of each class.\n",
    "\n",
    "The objective function of a linear SVM can be written as:\n",
    "\n",
    "      minimize (1/2) ||w||^2\n",
    "    where w is the weight vector, and ||w|| is its Euclidean norm.\n",
    "\n",
    "The objective function represents the trade-off between maximizing the margin and minimizing the classification error. The first term, (1/2) ||w||^2, is the regularization term that penalizes large values of w and encourages a smaller margin. The second term, the classification error, is zero if all data points are correctly classified.\n",
    "\n",
    "The optimal hyperplane is found by solving the constrained optimization problem:\n",
    "\n",
    "minimize (1/2) ||w||^2\n",
    "subject to y_i(w^T x_i + b) >= 1 for all i\n",
    "where y_i is the class label for the i-th training sample, x_i is the i-th training sample, and b is the bias term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0c931-93b1-453d-9279-4bdf20ec8652",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the kernel trick in SVM?\n",
    "Ans-\n",
    "    The kernel trick is a technique used in Support Vector Machine (SVM) algorithms to transform non-linearly separable data into a higher-dimensional feature space where it can be more easily separated by a linear classifier. It allows SVMs to efficiently classify complex datasets that would be difficult to classify using a linear classifier in the original input space.\n",
    "\n",
    "In the kernel trick, we use a kernel function to compute the dot product between two feature vectors in a higher-dimensional space without actually computing the coordinates of the vectors in that space. This allows us to compute the decision boundary in the higher-dimensional space without actually computing the coordinates of the data points in that space.\n",
    "\n",
    "The most commonly used kernel functions are the linear kernel, polynomial kernel, radial basis function (RBF) kernel, and sigmoid kernel. The choice of kernel function depends on the nature of the data and the complexity of the decision boundary needed.\n",
    "\n",
    "The kernel trick allows SVMs to learn non-linear decision boundaries in a computationally efficient way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273d53c-65ce-497e-af7b-37cd971edb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "Ans-\n",
    "    In Support Vector Machine (SVM) algorithms, support vectors are the data points that lie closest to the decision boundary (hyperplane) and have a non-zero weight in the solution vector. They play a crucial role in determining the decision boundary and the margin of the SVM classifier.\n",
    "\n",
    "The support vectors define the margin of the SVM classifier, which is the distance between the decision boundary and the closest data points from each class. The SVM classifier tries to maximize this margin by finding the hyperplane that separates the data points of different classes with the maximum margin.\n",
    "\n",
    "The support vectors also help to generalize the SVM classifier to new, unseen data. This is because the decision boundary of the SVM classifier is defined only by the support vectors, and not by all the training data points. The SVM classifier is therefore less sensitive to noise and outliers in the training data.\n",
    "\n",
    "For example, consider a binary classification problem where we have two classes of data points that are not linearly separable in 2D input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f87d85-9bf7-4230-85f4-5e8bb99f0489",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "Ans-\n",
    "    1. Hyperplane: The hyperplane is the decision boundary that separates the data points of different classes in a binary classification problem. In a two-dimensional feature space, the hyperplane is a line. In a three-dimensional feature space, the hyperplane is a plane. In higher-dimensional feature spaces, the hyperplane is a hyperplane\n",
    "    \n",
    "    2. Marginal plane: The marginal plane is the plane that is parallel to the hyperplane and passes through the support vectors. It defines the margin of the SVM classifier, which is the distance between the hyperplane and the closest data points from each class.\n",
    "    \n",
    "    3.Hard margin: In a hard-margin SVM, the SVM classifier tries to find the hyperplane that separates the data points of different classes with the maximum margin, subject to the constraint that all data points are classified correctly. This means that there is no allowance for misclassification, and the decision boundary is rigid.\n",
    "    \n",
    "    4.Soft margin: In a soft-margin SVM, the SVM classifier allows for some misclassification to find a decision boundary that is less rigid and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48efec0c-402e-40fd-8565-9f4681c25210",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "\n",
    "Ans-\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data[:, :2], iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the SVM model class\n",
    "class LinearSVM:\n",
    "    def __init__(self, lr=0.01, num_iters=1000, C=1.0):\n",
    "        self.lr = lr\n",
    "        self.num_iters = num_iters\n",
    "        self.C = C\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize weights and bias\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        self.b = 0\n",
    "        \n",
    "        # Gradient descent optimization\n",
    "        for i in range(self.num_iters):\n",
    "            # Compute the gradient of the loss function\n",
    "            dw, db = self.gradient(X, y)\n",
    "            \n",
    "            # Update the weights and bias\n",
    "            self.w -= self.lr * dw\n",
    "            self.b -= self.lr * db\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # Compute the decision function\n",
    "        y_pred = np.dot(X, self.w) + self.b\n",
    "        \n",
    "        # Classify the samples\n",
    "        y_pred[y_pred >= 0] = 1\n",
    "        y_pred[y_pred < 0] = 0\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        # Compute the gradient of the loss function\n",
    "        y_score = np.dot(X, self.w) + self.b\n",
    "        margins = y * y_score\n",
    "        misclassified = np.where(margins < 1)[0]\n",
    "        dw = self.w - (self.C * np.sum(y[misclassified, np.newaxis] * X[misclassified], axis=0))\n",
    "        dw[misclassified] += self.C * y[misclassified] * X[misclassified]\n",
    "        db = -self.C * np.sum(y[misclassified])\n",
    "        \n",
    "        return dw, db\n",
    "\n",
    "# Train a linear SVM classifier on the training set\n",
    "model = LinearSVM(C=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Plot the decision boundaries of the trained model using two of the features\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plot_decision_regions(X_test, y_test, clf=model, legend=2)\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('sepal width (cm)')\n",
    "plt.title('Linear SVM Decision Boundaries')\n",
    "plt.show()\n",
    "This code trains a linear SVM classifier on the first two features of the Iris dataset, and uses gradient descent optimization to find the weights and bias that maximize the margin between the two classes. The C parameter controls the trade-off between maximizing the margin and minimizing the classification error. The code then predicts the labels for the testing set and computes the accuracy of the model on the testing set. Finally, it plots the decision boundaries of the trained model using the plot_decision_regions function from the mlxtend library.\n",
    "\n",
    "To compare the performance of the custom implementation with the scikit-learn implementation, we can use the `LinearSVC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
